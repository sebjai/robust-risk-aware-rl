# -*- coding: utf-8 -*-
"""solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YntpDCyNgViEfj1HRkG4fiA0sHc5Lini
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.distributions.empirical_distribution import ECDF
from metrics import *

# 'Adversary' class controls the inner problem
# Given a set of parameters, X_phi strategy and market model from the 'Agent' class, the 'Adversary' class aims to find the network that produces the best/worst case distortion of X_phi
class Adversary():
  def __init__(self, rm_params, lm_params, wass_params, train_params, device, inner_net, net_name, objective, reset_lm, reset_net):
    self.rm_params = rm_params
    self.lm_params = lm_params
    self.wass_params = wass_params
    self.train_params = train_params
    self.device = device
    # inner_net distorts X_phi 
    self.inner_net = inner_net
    torch.save(self.inner_net.state_dict(), "initial_inner")
    self.net_name = net_name
    _, _, _, _, inner_lr, _, _, _ = self.train_params.GetParams()
    self.optimizer = optim.Adam(self.inner_net.parameters(), lr = inner_lr)
    # objective can either be 'maximize' or 'minimize' corresponding to the sup or inf of the inner problem 
    self.objective = objective
    self.reset_lm = reset_lm
    self.reset_net = reset_net
    # track training iterations
    self.update_count = 0
    # initialize history of metrics/variables to track
    self.InitHistory()
    
  # sim_theta uses self.inner_net to return terminal X_theta scenarios as a function of terminal X_phi and other sources of randomness coming from the market model
  def sim_theta(self, X_phi_T, market_model):
    # needs to be overrided as X_theta simulations are problem dependent 
    raise Exception("Must be overridden in subclass.")
  
  # X_phi_T is controlled by the "Agent" class' outer network, and has gradients detached when passed into the "Adversary" class 
  def train(self, X_phi_T, market_model):
    # option to reset langrange multipliers after adversary is trained
    if self.reset_lm:
      self.lm_params.ResetParams()

    # option to reset inner network after adversary is trained
    if self.reset_net:
      #self.inner_net.apply(lambda m :  m.reset_parameters() if isinstance(m, nn.Linear) else None)
      self.inner_net.load_state_dict(torch.load("initial_inner"))
      self.InitHistory()
      self.update_count = 0

    inner_epochs, _, plot_freq_inner, _, _, _, _, _ = self.train_params.GetParams()
    for i in range(inner_epochs):
      self.optimizer.zero_grad()
      # sim_theta returns terminal X_theta scenarios
      X_theta_T = self.sim_theta(X_phi_T, market_model).reshape(-1, 1)
      # calculate loss (grad), risk measure of X_theta (no grad) and wasserstein distance (no grad) between X_theta and X_phi
      loss, _, rm_theta, wass_dist = GetMetrics(X_phi_T, X_theta_T, self.rm_params, self.lm_params, self.wass_params, rm_objective = self.objective, problem_type = "inner", device = self.device)
      loss.backward()
      self.optimizer.step()
      self.UpdateHistory(loss.item(), rm_theta, wass_dist)
      self.update_count += 1

      # update augmented lagrangian parameters every 'update_freq' number of training iterations
      lam, mu, update_freq = self.lm_params.GetParams()
      if self.update_count % update_freq == 0:
        self.UpdateLM()

      if (i + 1) % plot_freq_inner == 0:
        # print most recent metrics
        print("Inner Epoch: {}".format(i))
        self.PlotCustom(X_phi_T, X_theta_T)
    
    return self.inner_net

  def UpdateLM(self):
    _, wass_limit = self.wass_params.GetParams()
    # update rule is dependent on the average constraint error from recent 5 iterations
    lookback = min(len(self.wass_dist_history), 5)
    lam, mu = self.lm_params.UpdateParams(np.mean(np.array(self.wass_dist_history[-lookback:]) - wass_limit))
    # update lagrangian multiplier history
    self.lam_history.append(lam)

  # initialize history of the inner problem loss, risk measure of X_theta, wasserstein distance between X_theta and X_phi, and the lagrange multiplier
  def InitHistory(self):
    self.loss_history = []
    self.rm_theta_history = []
    self.wass_dist_history = []
    self.lam_history = [self.lm_params.params["lam"]]
    return
  
  # update all history except for lagrangian multiplier
  def UpdateHistory(self, loss, rm_theta, wass_dist):
    self.loss_history.append(loss)
    self.rm_theta_history.append(rm_theta)
    self.wass_dist_history.append(wass_dist)
    return

  def PrintMetrics(self):
    lookback = min(len(self.wass_dist_history), 5)
    print("Wass Dist History: ", self.wass_dist_history[-lookback:])
    print("Risk Measure X_theta History: ", self.rm_theta_history[-lookback:])
    print("Loss History: ", self.loss_history[-lookback:])
    lam, mu, _ = self.lm_params.GetParams()
    print("Augmented Lagrangian lambda: {} mu: {}".format(lam, mu))
    return

  # may require different plots depending on problem
  # option to override in subclass 
  def PlotCustom(self, X, Y):
    pass

  def PlotHistory(self):
    plt.figure(figsize=(15 ,5))
    # first subplot is R[X_theta] history 
    plt.subplot(1, 3 ,1)
    plt.plot(self.rm_theta_history)
    plt.xlabel("Iteration", fontsize = 20)
    plt.ylabel(r'$R[X_{\theta}]$', fontsize = 20)
    # second subplot is wass distance history
    plt.subplot(1, 3, 2)
    plt.plot(self.wass_dist_history)
    plt.xlabel("Iteration", fontsize = 20)
    plt.ylabel("Wass Distance", fontsize = 20)
    # third subplot is lagrange multiplier history
    plt.subplot(1, 3, 3)
    plt.plot(self.lam_history)
    plt.xlabel("Iteration", fontsize = 20)
    plt.ylabel(r'$\lambda$', fontsize = 20)
    
    plt.tight_layout()
    plt.show()

  def PlotDistortion(self, X, Y):
    plt.figure(figsize=(15,5))
    a, b, p, rm_type = self.rm_params.GetParams()
    X = X.cpu().detach().numpy().squeeze()
    Y = Y.cpu().detach().numpy().squeeze()

    # first subplot is distribution plots of both terminal X_phi and terminal X_theta
    ax1 = plt.subplot(1, 3, 1)
    sns.distplot(X, hist=True, kde= True, label= r'$X^{\phi}$')
    sns.distplot(Y, hist=True, kde= True, label= r'$X^{\theta}$')
    plt.axvline(np.mean(X[X<= np.quantile(X, a)]), color='k', ls = '--', alpha = 0.35)
    plt.axvline(np.mean(Y[Y<= np.quantile(Y, a)]), color='r', ls = '--', alpha = 0.35)

    if rm_type == 'alpha-beta':
      plt.axvline(np.mean(X[X> np.quantile(X, b)]), color='k', ls = '--', alpha = 0.35)
      plt.axvline(np.mean(Y[Y> np.quantile(Y, b)]), color='r', ls = '--', alpha = 0.35)
    
    elif rm_type == 'custom':
      plt.axvline(np.mean(X), color='k', ls = '--', alpha = 0.35)
      plt.axvline(np.mean(Y), color='r', ls = '--', alpha = 0.35)

    plt.xlabel(r'$X_{T}$', fontsize = 20)
    plt.legend(loc = "upper right", fontsize = 20)

    # second subplot is scatter between terminal X_phi on the x-axis and terminal X_theta on the y-axis
    ax2 = plt.subplot(1, 3, 2)
    base = [min(X), max(X)]
    plt.plot(base, base, '--k')
    plt.scatter(X, Y, marker='o', color='r', s= 0.5, alpha = 0.25)
    ax2.xaxis.set_major_locator(plt.MaxNLocator(4))
    ax2.yaxis.set_major_locator(plt.MaxNLocator(4))
    # plt.ylim(0.9 * np.quantile(X, 0.01), 1.1 * np.quantile(X, 0.99))
    # plt.xlim(0.9 * np.quantile(X, 0.01), 1.1 * np.quantile(X, 0.99))
    plt.xlabel(r'$X_{T}^{\phi}$', fontsize = 20)
    plt.ylabel(r'$X_{T}^{\theta}$', fontsize = 20)

    # third subplot is a quantile plot of both terminal X_phi and terminal X_theta
    ax3 = plt.subplot(1, 3, 3)
    ecdf_Y = ECDF(Y)      
    U_Y = ecdf_Y(Y)
    ecdf_X = ECDF(X)     
    U_X = ecdf_X(X)
    plt.scatter(U_Y, Y, label = r"$X^{\theta}$ ", s= 0.5, alpha = 0.2)
    plt.plot(np.sort(U_X), np.sort(X), '--k', label = r"$X^{\phi}$")
    plt.xlabel("Quantiles", fontsize = 20)
    plt.ylabel(r'$X_{T}$', fontsize = 20)
    lgnd = plt.legend(loc = "upper left", fontsize = 20)
    lgnd.legendHandles[1].set_sizes([8,0])
    lgnd.legendHandles[1].set_alpha(1)
    
    plt.tight_layout()
    plt.show()

# 'Agent' class controls the outer problem
# Given a set of parameters including the market model and an 'Adversary' that can find the best/worst case distortion, the agent aims to find the best robust strategy
# In the special case of Problem 2 in the paper, the agent has a fixed benchmark strategy, and as such, the self.requires_update flag will be set to False
# In Problem 2, the 'Agent' will simulate X_phi within the market model to train the 'Adversary', without training the phi strategy
class Agent():
  def __init__(self, market_model, sim_params, train_params, device, outer_net, net_name, requires_update):
    self.market_model = market_model
    self.sim_params = sim_params
    self.train_params = train_params
    self.device = device
    # boolean indicating if Agent needs to be trained
    self.requires_update = requires_update
    # outer_net seeks a robust strategy over the market model
    self.outer_net = outer_net
    self.net_name = net_name
    if self.requires_update:
      _, _, _, _, _, outer_lr, _, _ = self.train_params.GetParams()
      self.optimizer = optim.Adam(self.outer_net.parameters(), lr = outer_lr)

    _, _, Nsims, _, _, _, _ = self.sim_params.GetParams()
    self.X_theta_history = np.empty((Nsims, 0))
    # other sources of loss not pertaining to risk measure, such as inventory
    self.other_loss = torch.zeros(1).to(device)
    

 # in Problems 1 and 3, sim_phi uses self.outer_net to return terminal X_phi scenarios as a function of the market model random variables encapsulated in self.market_model
 # in Problem 2, the phi strategy is fixed and so outer_net is unnecessary. X_phi will then be simulated within the market model
 # in problems with other sources of loss dependent on X_phi, such as inventory loss in Problem 3, sim_phi will update self.other_loss
  def sim_phi(self):
    # needs to be overrided as X_phi simulations are problem dependent
    raise Exception("Must be overridden in subclass.")

  # find X_theta_T given X_phi_T and adversary, backpropogate outer loss if necessary
  def step_theta(self, X_phi_T, adversary):

    if self.requires_update:
      self.optimizer.zero_grad()
      # freeze the inner network 
      for param in adversary.inner_net.parameters():
        param.requires_grad = False
      
      # use trained inner network to distort X_phi
      # gradients are attached only from X_phi_T and not the inner network
      X_theta_T = adversary.inner_net(X_phi_T).reshape(-1, 1)

      # other_loss is set to 0 for Problem 1, but includes the inventory loss for Problem 3
      rm_loss, rm_phi, rm_theta, _ = GetMetrics(X_phi_T, X_theta_T, adversary.rm_params, adversary.lm_params, adversary.wass_params,
                                                rm_objective = "minimize", problem_type = "outer", device = self.device)
      self.loss = rm_loss + self.other_loss
      self.loss.backward()
      self.optimizer.step()

      # unfreeze inner network
      for param in adversary.inner_net.parameters():
        param.requires_grad = True
    
    else:
      # find X_theta_T but no backpropogation step as no training is required for the outer network
      X_theta_T = adversary.sim_theta(X_phi_T, self.market_model).reshape(-1, 1)
      rm_theta = GetRiskMeasure(X_theta_T, adversary.rm_params).item()
      rm_phi = GetRiskMeasure(X_phi_T, adversary.rm_params).item()

    self.UpdateHistory(rm_theta, rm_phi)
    self.X_theta_history = np.concatenate((self.X_theta_history, X_theta_T.cpu().detach().numpy()), axis = 1)
    return X_theta_T
  
  def train(self, adversary):
    self.InitHistory()
    _, outer_epochs, _, plot_freq_outer, _, _, freeze_market_iter, freeze_inner_iter = self.train_params.GetParams()
    for i in range(outer_epochs):
      self.market_model.Sim(self.sim_params)
      # freeze market randomness for 'freeze_market_iter' iterations
      for j in range(freeze_market_iter):
        X_phi_T = self.sim_phi().reshape(-1, 1)
        curr_iter = i * freeze_market_iter + j
        # freeze inner network for 'freeze_inner_iter' iterations before retraining inner_net using the previous inner_net weights as a starting point
        if curr_iter % freeze_inner_iter == 0:
          # remove gradients from X_phi_T before passing into adversary
          adversary.train(X_phi_T.detach(), self.market_model)
          torch.save(adversary.inner_net.state_dict(), adversary.net_name + "_Epoch_{}_Iter_{}".format(i, j))
        # find X_theta_T with trained adversary
        X_theta_T = self.step_theta(X_phi_T, adversary)
        if curr_iter % plot_freq_outer == 0:
          print("Outer Epoch: {} Outer Iter: {}".format(i, j))
          self.PlotCustom(X_phi_T, X_theta_T, adversary)
          if self.requires_update:
            torch.save(self.outer_net.state_dict(), self.net_name + "_Epoch_{}_Iter_{}".format(i, j))
        
    return self.outer_net

  def InitHistory(self):
    self.other_loss_history = []
    self.rm_theta_history = []
    self.rm_phi_history = []
    return
    
  def UpdateHistory(self, rm_theta, rm_phi):
    self.other_loss_history.append(self.other_loss.item())
    self.rm_theta_history.append(rm_theta)
    self.rm_phi_history.append(rm_phi)
    return

  # depending on problem, may require different plots
  # option to override in subclass 
  def PlotCustom(self, X, Y, adversary):
    pass

  def PlotHistory(self):
    plt.figure(figsize=(15 ,5))
    # first subplot is outer loss history
    plt.subplot(1, 3, 1)
    plt.plot(self.other_loss_history)
    plt.xlabel("Iteration", fontsize = 20)
    plt.ylabel("Other Loss", fontsize = 20)
    # second subplot is R[X_theta] history 
    plt.subplot(1, 3, 2)
    plt.plot(self.rm_theta_history)
    plt.xlabel("Iteration", fontsize = 20)
    plt.ylabel(r'$R[X_{\theta}]$', fontsize = 20)
    # third subplot is R[X_phi] history 
    plt.subplot(1, 3, 3)
    plt.plot(self.rm_phi_history)
    plt.xlabel("Iteration", fontsize = 20)
    plt.ylabel(r'$R[X_{\phi}]$', fontsize = 20)
    
    plt.tight_layout()
    plt.show()